# Session 3: Three-Layer Knowledge Model + Curriculum Generator (2026-02-19)

## Overview

Implemented the full three-layer knowledge model and curriculum generator across 6 sprints. The previous knowledge model tracked items with a flat mastery state machine and two FSRS states. This session expanded it into a multi-dimensional system that tracks vocabulary, grammar, pragmatics, context breadth, and modality coverage — with a curriculum generator that implements Krashen's i+1 theory on top.

Every module contains real logic. No stubs.

---

## What Was Built

### Sprint 1: Database Schema Expansion + Shared Types

**Prisma schema (`prisma/schema.prisma`)** — Expanded from 6 models to 9:

- **LearnerProfile** gained 14 new fields: computed JLPT levels (`computedLevel`, `comprehensionCeiling`, `productionCeiling`), per-modality skill levels (`readingLevel`, `writingLevel`, `listeningLevel`, `speakingLevel`), session/streak tracking (`totalSessions`, `totalReviewEvents`, `currentStreak`, `longestStreak`, `lastActiveDate`), and aggregated pattern summaries (`errorPatternSummary`, `avoidancePatternSummary`)
- **LexicalItem** gained 9 new fields: context tracking (`contextTypes[]`, `contextCount`), per-modality counters (`readingExposures`, `listeningExposures`, `speakingProductions`, `writingProductions`), reference data (`frequencyRank`, `jlptLevel`), and accumulated `productionWeight`
- **GrammarItem** gained matching context/modality fields plus `novelContextCount` (transfer evidence) and `prerequisiteIds[]` (dependency chain)
- **ReviewEvent** gained `productionWeight` (0.5 for drill, 1.0 for conversation) and `contextType`
- **ItemContextLog** (new) — Tracks every context encounter per item with `contextType`, `modality`, `wasProduction`, `wasSuccessful`, `contextQuote`, `sessionId`. Indexed on `[lexicalItemId, timestamp]` and `[grammarItemId, timestamp]`
- **PragmaticProfile** (new) — Singleton for Layer 3: register accuracy metrics, communication strategy counts, V2 voice placeholders, avoided pattern lists
- **CurriculumItem** (new) — Queued recommendations with `priority`, `reason`, `status` (queued/introduced/skipped). Indexed on `[status, priority]`

All new fields use `@default()`, all new models are additive. Zero data loss on migration.

**Shared types (`shared/types.ts`)** — Added ~150 lines:

- New base types: `LearningModality`, `ContextType`
- 12 new IPC channels: `PROFILE_GET/UPDATE/RECALCULATE`, `CURRICULUM_GET_BUBBLE/GET_RECOMMENDATIONS/INTRODUCE_ITEM/SKIP_ITEM/REGENERATE`, `PRAGMATIC_GET_STATE/UPDATE`, `CONTEXT_LOG_LIST/ADD`
- New interfaces: `ExpandedLearnerProfile`, `KnowledgeBubble`, `LevelBreakdown`, `CurriculumRecommendation`, `PragmaticState`, `ContextLogEntry`, `ExpandedItemDetail`, `ExpandedTomBrief`, `ExpandedSessionPlan`, `ExpandedPostSessionAnalysis`
- Extended `ReviewSubmission` with optional `productionWeight` and `contextType`

---

### Sprint 2: Profile Calculator + Mastery Expansion

**`core/profile/calculator.ts`** (new) — `recalculateProfile()`:

- Groups items by JLPT level (N5-N1)
- Computes `comprehensionCeiling`: highest level where avg recognition FSRS retrievability > 0.80
- Computes `productionCeiling`: highest level where avg production FSRS retrievability > 0.60
- `computedLevel` = max of the two ceilings
- `readingLevel` / `writingLevel` = weighted avg retrievability across reviewed items
- Streak logic: consecutive day tracking with gap reset

**`core/mastery/state-machine.ts`** — Expanded `MasteryContext` with three new fields and three new promotion gates:

| Transition | New Gate | Requirement |
|---|---|---|
| apprentice_4 -> journeyman | `productionWeight` | >= 1.0 (one conversation production, or two drill productions at 0.5 each) |
| journeyman -> expert | `contextCount` | >= 3 distinct context types |
| expert -> master | `novelContextCount` | >= 2 novel contexts (grammar only, transfer evidence) |

**`electron/ipc/profile.ts`** (new) — Three handlers: `PROFILE_GET`, `PROFILE_UPDATE`, `PROFILE_RECALCULATE`

**`electron/ipc/reviews.ts`** — Expanded to:

- Accept/store `productionWeight` and `contextType` on ReviewEvent
- Create `ItemContextLog` entry on each review
- Increment per-modality counters (`readingExposures` or `writingProductions`)
- Update item `contextTypes[]` and `contextCount`
- Accumulate `productionWeight` on items
- Pass expanded context to `computeNextMasteryState`
- Trigger async profile recalculation every 10 reviews

---

### Sprint 3: Context Logging + Expanded ToM

**`electron/ipc/context-log.ts`** (new) — Two handlers:

- `CONTEXT_LOG_LIST`: paginated context logs for an item
- `CONTEXT_LOG_ADD`: creates log entry, updates item's `contextTypes` and `contextCount`, increments `novelContextCount` for grammar items seen in new contexts

**`core/tom/analyzer.ts`** — Added two new detectors:

- `detectModalityGap(items)`: computes avg retrievability per modality dimension (reading, writing, listening, speaking), flags gaps > 0.2 from the strongest modality
- `detectTransferGap(grammarItems)`: finds journeyman+ grammar items seen in fewer than 3 distinct contexts

Added `generateExpandedDailyBrief()`: runs all 5 detectors (avoidance, confusion pairs, regression, modality gap, transfer gap), incorporates pragmatic state, produces `ExpandedTomBrief` with modality gaps, transfer gaps, pragmatic insights, and curriculum suggestions.

**`electron/ipc/tom.ts`** — Expanded to:

- Query modality exposure data and grammar transfer data
- Query PragmaticProfile
- Call `generateExpandedDailyBrief()` instead of `generateDailyBrief()`
- Store `modality_gap` and `transfer_gap` type inferences
- Update `LearnerProfile.errorPatternSummary` and `avoidancePatternSummary`
- Properly resolve old inferences before creating new ones (fixed the id=0 upsert hack)

---

### Sprint 4: Curriculum Generator

**`core/curriculum/data/japanese-reference.json`** (new) — Static reference corpus: ~200 vocabulary items + ~50 grammar patterns across N5-N1, with real Japanese words, accurate readings/meanings, realistic frequency ranks, and a valid prerequisite dependency graph for grammar patterns.

**`core/curriculum/reference-data.ts`** (new) — Loads/caches reference corpus, filters by JLPT level or frequency range.

**`core/curriculum/bubble.ts`** (new) — `computeKnowledgeBubble()`:

1. Groups learner items by JLPT level
2. For each level: counts reference items, known items (mastery >= apprentice_3), production-ready items (mastery >= journeyman)
3. Coverage = known / total per level
4. `currentLevel` = highest level where coverage >= 0.80
5. `frontierLevel` = next level up (the i+1)
6. `gapsInCurrentLevel` = weak items + missing reference items at current level

Also: `identifyGaps()` returns prioritized gap analysis with severity levels (high/medium/low).

**`core/curriculum/recommender.ts`** (new) — `generateRecommendations()`:

Scoring algorithm per candidate from the frontier + current level:
- `frequencyScore`: `1.0 / log2(frequencyRank + 2)` — higher frequency = higher score
- `gapScore`: +0.5 if fills a gap in current level
- `dependencyScore`: +0.3 if all grammar prerequisites met, -2.0 if not
- `tomScore`: -0.3 in areas with active regression, +0.2 if fills avoidance gap
- `contextScore`: +0.1 for high-frequency items

Sorted by composite score, capped at `dailyNewItemLimit`. Includes `checkPrerequisites()` for grammar dependency validation.

**`electron/ipc/curriculum.ts`** (new) — Five handlers:

- `CURRICULUM_GET_BUBBLE`: computes and returns KnowledgeBubble
- `CURRICULUM_GET_RECOMMENDATIONS`: runs full engine, stores results as CurriculumItem rows
- `CURRICULUM_INTRODUCE_ITEM`: creates LexicalItem/GrammarItem from recommendation, marks "introduced"
- `CURRICULUM_SKIP_ITEM`: marks as "skipped"
- `CURRICULUM_REGENERATE`: clears queued items, re-runs engine with fresh data

---

### Sprint 5: Pragmatics Analyzer (Layer 3)

**`core/pragmatics/analyzer.ts`** (new):

- `buildPragmaticAnalysisPrompt()`: builds a Claude prompt to analyze register usage (casual/polite accuracy), circumlocution (positive communication strategy), L1 fallbacks, silence/avoidance events, and avoided grammar patterns
- `parsePragmaticAnalysis()`: parses Claude's JSON response into typed result
- `updatePragmaticState()`: merges new analysis into running state using exponential moving average (alpha=0.3) for accuracy metrics, cumulative sums for event counts

**`electron/ipc/pragmatics.ts`** (new) — Two handlers: `PRAGMATIC_GET_STATE` (with upsert for initial creation), `PRAGMATIC_UPDATE`

---

### Sprint 6: Conversation Integration

**`core/conversation/planner.ts`** — Major expansion:

- `LearnerSummary` gained: `comprehensionCeiling`, `productionCeiling`, `modalityGap`, `pragmaticState`, `curriculumNewItems`
- `buildPlanningPrompt()` now includes sections for curriculum recommendations, modality gaps, pragmatic state, and transfer test targets. Requests `pragmatic_targets`, `curriculum_new_items`, and `transfer_test_targets` in the JSON response
- Added `parseSessionPlan()` for parsing Claude's response into `ExpandedSessionPlan`
- `buildConversationSystemPrompt()` gained 4 new behavioral rules: track register slips, note circumlocution positively, naturally introduce i+1 items, create novel contexts for transfer testing

**`core/conversation/analyzer.ts`** — Expanded:

- `buildAnalysisPrompt()` now requests register accuracy, circumlocution/L1 fallback/silence events, and per-item context logs
- `parseAnalysis()` returns `ExpandedPostSessionAnalysis` with `registerAccuracy`, `strategyEvents`, and `contextLogs`

**`electron/ipc/conversation.ts`** — Full implementation replacing all stubs:

- `CONVERSATION_PLAN`: queries expanded profile -> runs curriculum -> generates expanded ToM brief -> calls Claude API -> parses ExpandedSessionPlan -> creates ConversationSession -> increments totalSessions
- `CONVERSATION_SEND`: maintains in-memory session state, builds API messages from last 30 turns, calls Claude API with system prompt, updates transcript in DB
- `CONVERSATION_END`: full post-session pipeline:
  1. Runs post-session analysis via Claude API
  2. Creates ItemContextLog entries for each context log
  3. Updates item modality counters and context types
  4. Updates production weights and counts
  5. Adds newly encountered items as "introduced"
  6. Runs pragmatic analysis via separate Claude API call
  7. Updates PragmaticProfile with EMA
  8. Triggers profile recalculation

**`electron/main.ts`** — Registers all 8 IPC handler groups (was 4).

---

## Files Modified (14)

| File | Changes |
|---|---|
| `prisma/schema.prisma` | Expanded 4 models, added 3 new models |
| `shared/types.ts` | ~150 lines of new types + 12 IPC channels |
| `core/mastery/state-machine.ts` | Expanded context, 3 new promotion gates |
| `core/mastery/index.ts` | Re-exported (unchanged interface) |
| `core/tom/analyzer.ts` | 2 new detectors, expanded daily brief |
| `core/tom/index.ts` | New exports |
| `core/conversation/planner.ts` | Expanded LearnerSummary, prompts, added parser |
| `core/conversation/analyzer.ts` | Expanded analysis prompt + parser |
| `core/conversation/index.ts` | New exports |
| `electron/main.ts` | 4 new IPC handler registrations |
| `electron/ipc/reviews.ts` | Context logging, modality counters, production weight, profile recalc |
| `electron/ipc/tom.ts` | Expanded queries, new detectors, fixed upsert logic |
| `electron/ipc/conversation.ts` | Full implementation replacing all stubs |
| `tsconfig.node.json` | Added `core/**/*.json` to include |

## Files Created (14)

| File | Purpose |
|---|---|
| `core/profile/index.ts` | Barrel export |
| `core/profile/calculator.ts` | Profile recalculation engine |
| `core/curriculum/index.ts` | Barrel export |
| `core/curriculum/bubble.ts` | Knowledge bubble computation |
| `core/curriculum/recommender.ts` | i+1 recommendation engine |
| `core/curriculum/reference-data.ts` | Reference corpus loader |
| `core/curriculum/data/japanese-reference.json` | ~200 vocab + ~50 grammar across N5-N1 |
| `core/pragmatics/index.ts` | Barrel export |
| `core/pragmatics/analyzer.ts` | Pragmatic competence analysis |
| `electron/ipc/profile.ts` | Profile CRUD + recalculation handler |
| `electron/ipc/curriculum.ts` | Curriculum bubble, recommendations, introduce/skip |
| `electron/ipc/pragmatics.ts` | Pragmatic state get/update |
| `electron/ipc/context-log.ts` | Context log list/add |

---

## Verification

- `npx prisma generate` — Prisma client regenerated successfully
- `npx tsc --noEmit -p tsconfig.node.json` — PASS
- `npx tsc --noEmit -p tsconfig.web.json` — PASS
- `npm run build` — Production build succeeds (clean, no warnings)

---

## Issues Encountered & Resolved

1. **Prisma client stale after schema changes** — All `Property 'X' does not exist on type 'PrismaClient'` errors. Fixed by running `npx prisma generate` to regenerate the client from the updated schema.
2. **JSON file not in tsconfig include** — `core/curriculum/data/japanese-reference.json` not found during compilation. Fixed by adding `core/**/*.json` to `tsconfig.node.json` include patterns.
3. **Prisma JSON type casting** — `Record<string, unknown>` not assignable to `Prisma.InputJsonValue`. Fixed by casting through `as unknown as Prisma.InputJsonValue` (matching existing codebase pattern).
4. **Dynamic import warning** — `core/profile/calculator.ts` was dynamically imported in `reviews.ts` but statically imported elsewhere, triggering a vite chunk warning. Fixed by switching to a static import.

---

## Current Project State

### What's Working
- Full three-layer knowledge model (vocabulary/grammar + context/modality + pragmatics)
- Curriculum generator with i+1 recommendations from static JLPT reference corpus
- Expanded mastery gates requiring production weight, context breadth, and transfer evidence
- Profile calculator with JLPT ceiling computation and streak tracking
- 5-detector ToM engine (avoidance, confusion, regression, modality gap, transfer gap)
- Full conversation pipeline: planning -> conversation -> post-session analysis -> pragmatic analysis -> profile update
- Context logging for every item interaction across all contexts
- 26 IPC channels across 8 domains, all with real implementations
- Clean TypeScript compilation and production build

### What's Not Yet Built
- Database migration (requires running Supabase — `npx prisma migrate dev --name expand_knowledge_model`)
- Review card UI (page exists but card interface not built)
- Onboarding/placement flow
- Word bank detail views with expanded item data
- Insights page showing ToM inferences, knowledge bubble, modality gaps
- Curriculum UI (recommendations, introduce/skip actions)
- Profile/settings page with level display
